<h2>Perl深層学習AI入門</h2>

Perl深層学習AI入門は、深層学習に必要な基礎をPerlのサンプルコードで学べる講座です。深層学習はディープラーニングとも呼ばれており高い精度を実現できる機械学習のひとつの分野です。深層学習に必要な数学的知識を、高校数学で学んでいる数学の基本的な知識だけで、学べるように工夫しています。数式をなるべく少なく、プログラムによるアルゴリズムを中心に紹介していることも特徴です。

Perlだけで書かれているため、専用のライブラリを準備したり、学んだりしなくても学習を始められることが、このサイトの特徴です。Perlをインストールすれば、すぐに深層学習の学習を始められます。

深層学習AIを使って、実務で実践的に、画像認識、パターン認識、音声認識をしたい方のために、Perlの深層学習ライブラリ「AI::NXNet」も紹介しています。

<ul>
  <li><a href="https://www.youtube.com/watch?v=dTKY0kor50A">動画紹介</a></li>
</ul>

<h3>深層学習とは</h3>

<ul>
  <li><a href="/blog/20210920074017.html">深層学習とは</a></li>
</ul>

<h3>深層学習を学ぶためのPerl環境構築</h3>

深層学習を学ぶためにPerlをインストールしましょう。Windows、Mac、Linux/UNIXに対応しています。

<ul>
  <li><a href="https://perlzemi.com/blog/20180820153471.html">Perlのインストール</a></li>
</ul>

<h3>Perlの深層学習のライブラリ</h3>

深層学習AIを使って、実務で実践的に、画像認識、パターン認識、音声認識をしたい方のために、Perlの深層学習ライブラリ「AI::NXNet」の紹介です。

<ul>
  <li><a href="/blog/20201118093000.html">Perlの深層学習のライブラリ - AI::MXNet</a></li>
</ul>

<h3>基礎知識</h3>

深層学習の基礎知識を学びましょう。

<ul>
  <li><a href="/blog/20200905120907.html">隠れ層におけるニューロンの数を表現する</a></li>
  <li><a href="/blog/20200904120907.html">エポックという単位は訓練データを一巡することを指す</a></li>
  <li><a href="/blog/20200830120907.html">バッチサイズとは - オンライン学習、ミニバッチ学習、バッチ学習</a></li>
  <li><a href="/blog/20200923123308.html">学習率とは</a></li>
  <li><a href="/blog/20200921123308.html">勾配(こうばい)とは</a></li>
</ul>

<h3>入力処理</h3>

<ul>
  <li><a href="/blog/20200907120907.html">MINIST画像情報を読み込む</a></li>
  <li><a href="/blog/20200909120907.html">MINISTラベル情報を読み込む</a></li>
  <li><a href="/blog/20200906120907.html">訓練データをランダムにシャッフルする</a></li>
</ul>

<h3>重みとバイアス</h3>

<ul>
  <li><a href="/blog/20201016143424.html">バイアスとは</a></li>
  <li><a href="/blog/20201015143424.html">重みとは</a></li>
  <li><a href="/blog/20200311113241.html">各層の重みとバイアスの初期値の設定方法</a></li>
  <li><a href="/blog/20201005144439.html">正規分布に従う乱数を求める</a></li>
  <li><a href="/blog/20201007144439.html">Xavierの初期値</a></li>
  <li><a href="/blog/20201006144439.html">Heの初期値</a></li>
</ul>

<h3>深層学習で使う数学</h3>

<h4>入出力</h4>

<ul>
  <li><a href="/blog/20200302113052.html">隠れ層における計算 - m個の入力をn個の出力に変換する</a></li>
  <li><a href="/blog/20200306113052.html">出力層における計算</a></li>
  <li><a href="/blog/20200915121719.html">深層学習で初期入力から最終出力を得る計算過程</a></li>
</ul>

<h4>ベクトル</h4>

<ul>
  <li><a href="/blog/20200913103640.html">ベクトルの和を求める</a></li>
  <li><a href="/blog/20200828120907.html">ベクトルの差を求める</a></li>
  <li><a href="/blog/20200924123308.html">ベクトルの内積の計算</a></li>
</ul>

<h4>行列</h4>

<ul>
  <li><a href="/blog/20200928161518.html">行列とは</a></li>
  <li><a href="/blog/20200929161518.html">列優先行列を作成する</a></li>
  <li><a href="/blog/20200912123308.html">行列の和を求める</a></li>
  <li><a href="/blog/20200829120907.html">行列の差を求める</a></li>
  <li><a href="/blog/20200914103640.html">行列の積の計算</a></li>
  <li><a href="/blog/20200917123308.html">転置行列を求める</a></li>
</ul>

<h4>微分</h4>

<ul>
  <li><a href="/blog/20201020085300.html">傾き(かたむき)とは</a></li>
  <li><a href="/blog/20201021085550.html">多段の関数の場合の傾きの求め方 - 合成関数の微分</a></li>
  <li><a href="/blog/20200919123308.html">導関数とは</a></li>
  <li><a href="/blog/20201026095954.html">逆誤差伝播法 - バックプロパゲーション 書き始め</a></li>
</ul>

<h3>活性化関数</h3>

<ul>
  <li><a href="/blog/20200902120907.html">活性化関数とは</a></li>
  <li><a href="/blog/20200903120907.html">シグモイド関数</a></li>
  <li><a href="/blog/20200920123308.html">シグモイド関数の導関数</a></li>
  <li><a href="/blog/20200911102242.html">ReLU関数</a></li>
  <li><a href="/blog/20201001161518.html">ReLU関数の導関数</a></li>
  <li><a href="/blog/20201019123741.html">tanh関数</a></li>
  <li><a href="/blog/20201018123741.html">tanh関数の導関数</a></li>
</ul>

<h3>出力層</h3>

<ul>
  <li><a href="/blog/20200916101844.html">期待される出力を確率で表現する</a></li>
  <li><a href="/blog/20201002161518.html">softmax関数</a></li>
  <li><a href="/blog/20200927161518.html">softmaxクロスエントロピーの導関数</a></li>
</ul>

<h3>損失関数</h3>

<ul>
  <li><a href="/blog/20200901120907.html">損失関数とは</a></li>
  <li><a href="/blog/20200910120907.html">二乗和誤差を求める</a></li>
  <li><a href="/blog/20200831120907.html">クロスエントロピー誤差</a></li>
</ul>

<h3>重みとバイアスの更新</h3>

<ul>
  <li><a href="/blog/20201023083657.html">パラメーター更新最適化アルゴリズムとは</a></li>
  <li><a href="/blog/20201017123741.html">確率的勾配降下法(SGD) - 重みとバイアスのパラメータの更新</a></li>
  <li><a href="/blog/20201022083657.html">Adam - SGDの改善</a></li>
</ul>

<h3>Perl+深層学習で手書き文字認識</h3>

<ul>
  <li><a href="/blog/20200908120907.html">Perl+深層学習で手書き文字認識</a></li>
  <li><a href="/blog/20200926161518.html">ピュアPerlで書いたMNIST手書き認識深層学習</a></li>
</ul>

<h3>SPVMを使った深層学習高速化</h3>

<ul>
  <li><a href="/blog/20210510142804.html">SPVMを使った深層学習高速化</a></li>
</ul>

<h3>Perl深層学習AI入門の関連講座</h3>

Perl深層学習AI入門の関連講座のご紹介。

<h4>Perlプロブログラミング入門</h4>

Perlプログラミングの基礎を学ぶなら、こちら。

<ul>
  <li><a href="https://perlzemi.com/">Perlゼミ - Perlプログラミング入門</a></li>
</ul>

<h4>深層学習の高速化技術</h4>

深層学習の高速化技術を学ぶならこちら。

<ul>
  <li><a href="https://c.perlzemi.com/">Perl XSユーザーのためのC言語入門</a></li>
  <li><a href="https://bind.perlzemi.com/">Perlバインディング入門 - C/C++/CudaをPerlから呼び出す</a></li>
</ul>

<h4>データ分析</h4>

データ分析は、深層学習AIとは、直接の関連性はありませんが、世間では誤解され、関連性が高いものとして紹介されています。「ビッグデータ」「AI」という言葉がバズワードとして、情報の表面に流通しており、投資詐欺の可能性が高いものがありますので、ご注意ください。実際のデータ分析を知って、誤解を解ければと願います。

<ul>
  <li><a href="https://datascience.perlzemi.com/">データ分析入門</a></li>
</ul>

<h4>深層学習の質問</h4>

仲間と交流をしたり、深層学習の質問ができる会員限定のPerlクラブ・フォーラムがあります。Twitterのリプライ・ダイレクトメッセージ、Youtubeのコメント欄も利用できます。

<ul>
  <li><a href="https://perlclub.net/forum">Perlクラブ・フォーラム</a>
  <li><a href="https://twitter.com/perlzemi">Twitter</a>
  <li><a href="https://www.youtube.com/channel/UCbeAS6ZXpSKqkzb-Nykb0ZQ">Youtube</a>
</ul>
